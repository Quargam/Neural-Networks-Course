Всем привет! Объявление насчет домашнего задания:
 - (50 очков) необходимо реализовать в виде класса многослойную нейронную сеть и обучить ее. Можно обучать ее на тех же точках, что и в примере ко второй лекции. Если захотите на более интересных данных, пишите, что-нибудь придумаем. В итоге, необходимо в модуль  __init__  передать в качестве аргумента количество слоев, функцию активации и кол-во и нейронов в каждом из них. можно для этого использовать словарь {layer_1: [2, 'sigmoid'], layer_2: ...}. На практике нейронку с кол-вом слоев > 5 создавать не рекомендую.
 - (20 очков) выписать все формулы для forward propagation и  backward propagation, проверить что все верно и потом приступать к коду.
 - (20 очков: за внедрение SGD 10 очков, еще 10 за внедрение дополнительных фич) в качестве оптимизатора предлагается выбрать что-то из того, что мы вчера рассматривали на лекции (rms_prop, momentum или adam). Оптимизатор тоже реализовать надо в виде отдельного класса, как в примерах, которые я вам показывал.
 - (10 очков) надо будет разделить данные на train и test часть и построить графики зависимости функции потерь от номера эпохи. Эпоха - это такое кол-во итераций обучения, в течение которого через нейронку прогоняется весь датасет. Пока мы работали с обычным градиентным спуском, у нас одна итерация - это и была одна эпоха. Если у нас датасет размера 100 и мы берем каждый раз батч по 10 объектов, то одна эпоха будет состоять из 10 итераций. Вот для каждой эпохи надо считать усредненную функцию потерь (усредняем по кол-ву итераций соответственно) и выводить на график. Ваша задача словами описать по итогам этот график (есть ли переобучение или нет).

Задание лучше выкладывать на гитхаб. Я не требую, чтобы он был приватным, но плагиат кода недопустим. При этом нормально, если вы будете советоваться друг с другом и помогать какими-то идеями. Если будут сложные вопросы, обращайтесь ко мне. Лучше не в личку, а в этот чат, чтобы все могли помочь/подсказать. Дедлайн: 24 июля 23:59 мск. Удачи!